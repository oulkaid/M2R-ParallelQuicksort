---
title: "Benchmarking Analysis for a Multi-Threaded Version of Quicksort"
author: "Authors: Oussama Oulkaid, ##, ##"  
date: "November, 2021"
geometry: margin=1.5cm
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction
The aim of this activity is to analyse the time spent by Quicksort, by pinpointing the parameters that might affect its performance (array size, number of cores on the machine, nature of other applications running at the same time, etc.).

To get started, compile the program by running:
```{bash, eval=FALSE}
make -C src/
```

## 2. Choices we made
<!-- sample sizes, script automation, etc. -->
The `run_benchmarking.sh` script have been modified so that the array size samples are chosen to be incremented by the same amount. And to simplify the experimentation process, the Perl script that build the csv file is now being run within the `run_benchmarking.sh`. Also, the txt files are not anymore preserved.

To launch an experiment you need to set the `START_SIZE`, `MAX_SIZE` and `STEP` constants as you want. Then, run:
```{bash, eval=FALSE}
./scripts/run_benchmarking.sh
```


```{r, results=FALSE, message=FALSE}
# The environment
library(tidyverse)
library(ggplot2)
library(reshape2)
```


```{r, echo=FALSE, results=FALSE, message=FALSE}
version
```

## 3. Build the Dataframe from file
```{r, message=FALSE}
#parameters: START_SIZE=0, MAX_SIZE=15000000, STEP=1000000
df <- read.csv("data/in_2021-11-20/measurements_01:44.csv",header=T)
df <- melt(df, id.vars="Size")
names(df)[2] <- "Type"
names(df)[3] <- "Time"
```

## 4. Plotting

```{r, message=FALSE}
ggplot(df, aes(Size, Time, colour=Type)) + 
  geom_point(size = 0.2) +
  stat_smooth(size = 0.7) +
  theme( plot.title = element_text(hjust = 0.5), legend.position = "top" ) +
  scale_x_continuous(breaks=c(8000000, 11000000))

```

**Comment:** For arrays with a size of up to 11000000 elements, the built-in method is the most rapid. Afterwards, the Threaded computing performs better. 
Also, for small array sizes (less than 8000000) the use of Parallel computing only slows down the speed, and we see that the Sequential sorting provides quicker results.

**Note:** This experiment was done on a virtual machine (4 cores assigned, 8 GB of RAM), and there was no other application running when the expirement was running.

**TODO: ** Use other machines to explore the effect of system capabilities on the overall performance (and maybe make conclusions upon the patterns found).















